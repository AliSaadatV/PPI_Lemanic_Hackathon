{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7ade83ad95649ecabc952ad86ac2353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_549f0545842642f09392744431bcf665",
              "IPY_MODEL_db2596b750d2404a89a00a7216641a37",
              "IPY_MODEL_debdbbc0df004e739daef28fede5345c"
            ],
            "layout": "IPY_MODEL_4079a39ef524467cae4ab5e7c713e74b"
          }
        },
        "549f0545842642f09392744431bcf665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7333432325bc4c669c2d6a3dcf918238",
            "placeholder": "​",
            "style": "IPY_MODEL_5241c618b305484ebfb5d14bfccc1f1f",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "db2596b750d2404a89a00a7216641a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a410ddee7b5d48c1b2c96a2b11ae8acf",
            "max": 810661813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3f7d70c7f5343a58469cb52e6ee4bdb",
            "value": 810661813
          }
        },
        "debdbbc0df004e739daef28fede5345c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de0f6c208d844151a8062af186f7109d",
            "placeholder": "​",
            "style": "IPY_MODEL_7e7317819bdf4a909bc89995a2c35882",
            "value": " 811M/811M [00:09&lt;00:00, 84.3MB/s]"
          }
        },
        "4079a39ef524467cae4ab5e7c713e74b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7333432325bc4c669c2d6a3dcf918238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5241c618b305484ebfb5d14bfccc1f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a410ddee7b5d48c1b2c96a2b11ae8acf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3f7d70c7f5343a58469cb52e6ee4bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de0f6c208d844151a8062af186f7109d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e7317819bdf4a909bc89995a2c35882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFPB2QItsAMQ",
        "outputId": "2afcbdf0-2b95-4a74-efc5-5ce0eedb034a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: rjieba in /usr/local/lib/python3.10/dist-packages (0.1.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate accelerate rjieba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, RoFormerForSequenceClassification, RoFormerTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from datasets import Dataset\n",
        "from evaluate import load"
      ],
      "metadata": {
        "id": "VKboTrVh4W4O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#max_len = 157"
      ],
      "metadata": {
        "id": "CMY_aMOq1RhZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "onxzUvbIsJhZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adaptyvbio/lemanic_2024.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW891KUOsYe1",
        "outputId": "9392f388-48f1-4f1f-9e72-cdd1a572917e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lemanic_2024' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the data sets from literature and experiment\n",
        "root = \"lemanic_2024/data\"\n",
        "path_exp_train = root + \"/experiment_train.csv\"\n",
        "path_exp_test =  root + \"/experiment_test.csv\"\n",
        "path_lit_train = root + \"/literature_train.csv\"\n",
        "path_lit_test =  root + \"/literature_test.csv\"\n",
        "\n",
        "experiment_train_df = pd.read_csv(path_exp_train).dropna(subset=\"VHorVHH\")\n",
        "experiment_test_df = pd.read_csv(path_exp_test).dropna(subset=\"VHorVHH\")\n",
        "literature_train_df = pd.read_csv(path_lit_train).dropna(subset=\"VHorVHH\")\n",
        "literature_test_df = pd.read_csv(path_lit_test).dropna(subset=\"VHorVHH\")\n",
        "\n",
        "experiment_train_df['ID'] = ['id' + str(i) for i in range(1, len(experiment_train_df)+1)]\n",
        "experiment_test_df['ID'] = ['id' + str(i) for i in range(1, len(experiment_test_df)+1)]\n",
        "literature_train_df['ID'] = ['id' + str(i) for i in range(1, len(literature_train_df)+1)]\n",
        "literature_test_df['ID'] = ['id' + str(i) for i in range(1, len(literature_test_df)+1)]"
      ],
      "metadata": {
        "id": "Bm0do7IwsZc9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare training seq\n",
        "expe_train_Hseqs = list(experiment_train_df[\"VHorVHH\"].values)\n",
        "expe_test_Hseqs = list(experiment_test_df[\"VHorVHH\"].values)\n",
        "lit_train_Hseqs = list(literature_train_df[\"VHorVHH\"].values)\n",
        "lit_test_Hseqs = list(literature_test_df[\"VHorVHH\"].values)\n",
        "\n",
        "def transform_string(s):\n",
        "\n",
        "    return ' '.join(s)\n",
        "\n",
        "expe_train_Hseqs = [transform_string(s) for s in expe_train_Hseqs]\n",
        "expe_test_Hseqs = [transform_string(s) for s in expe_test_Hseqs]\n",
        "lit_train_Hseqs = [transform_string(s) for s in lit_train_Hseqs]\n",
        "lit_test_Hseqs = [transform_string(s) for s in lit_test_Hseqs]\n",
        "\n",
        "#prepare labels\n",
        "y_lit_test = list(np.array(literature_test_df[\"Binds\"].values).astype(int))\n",
        "y_lit_train = list(np.array(literature_train_df[\"Binds\"].values).astype(int))\n",
        "y_exp_test = list(np.array(experiment_test_df[\"Binds\"].values).astype(int))\n",
        "y_exp_train = list(np.array(experiment_train_df[\"Binds\"].values).astype(int))"
      ],
      "metadata": {
        "id": "Zr6WcWCfscrP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer and model\n",
        "\n",
        "tokenizer = RoFormerTokenizer.from_pretrained('alchemab/antiberta2')#, truncation=True, max_length=max_len)\n",
        "model_exp = RoFormerForSequenceClassification.from_pretrained('alchemab/antiberta2', num_labels=2) #.resize_token_embeddings(len(tokenizer))\n",
        "model_lit = RoFormerForSequenceClassification.from_pretrained('alchemab/antiberta2', num_labels=2) #.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "e7ade83ad95649ecabc952ad86ac2353",
            "549f0545842642f09392744431bcf665",
            "db2596b750d2404a89a00a7216641a37",
            "debdbbc0df004e739daef28fede5345c",
            "4079a39ef524467cae4ab5e7c713e74b",
            "7333432325bc4c669c2d6a3dcf918238",
            "5241c618b305484ebfb5d14bfccc1f1f",
            "a410ddee7b5d48c1b2c96a2b11ae8acf",
            "b3f7d70c7f5343a58469cb52e6ee4bdb",
            "de0f6c208d844151a8062af186f7109d",
            "7e7317819bdf4a909bc89995a2c35882"
          ]
        },
        "id": "sjcSUbm5uSOt",
        "outputId": "7aa85503-c60c-4bb7-e2b2-afc9045dd205"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/811M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7ade83ad95649ecabc952ad86ac2353"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RoFormerForSequenceClassification were not initialized from the model checkpoint at alchemab/antiberta2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RoFormerForSequenceClassification were not initialized from the model checkpoint at alchemab/antiberta2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "expe_train_Hseqs_tokenized = tokenizer(expe_train_Hseqs)\n",
        "expe_test_Hseqs_tokenized = tokenizer(expe_test_Hseqs)\n",
        "\n",
        "lit_train_Hseqs_tokenized = tokenizer(lit_train_Hseqs)\n",
        "lit_test_Hseqs_tokenized = tokenizer(lit_test_Hseqs)"
      ],
      "metadata": {
        "id": "gV4sqyEuwNwb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataset\n",
        "expe_train_Hseqs_dataset = Dataset.from_dict(expe_train_Hseqs_tokenized).add_column(\"labels\", y_exp_train)\n",
        "expe_test_Hseqs_dataset = Dataset.from_dict(expe_test_Hseqs_tokenized).add_column(\"labels\", y_exp_test)\n",
        "\n",
        "lit_train_Hseqs_dataset = Dataset.from_dict(lit_train_Hseqs_tokenized).add_column(\"labels\", y_lit_train)\n",
        "lit_test_Hseqs_dataset = Dataset.from_dict(lit_test_Hseqs_tokenized).add_column(\"labels\", y_lit_test)"
      ],
      "metadata": {
        "id": "XcouImNA1psh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train arg\n",
        "batch_size = 8\n",
        "\n",
        "args_exp = TrainingArguments(\n",
        "    f\"AbLang-finetuned-exp\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "args_lit = TrainingArguments(\n",
        "    f\"AbLang-finetuned-lit\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=False,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False\n",
        ")"
      ],
      "metadata": {
        "id": "t2_KXT3q2iuv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics\n",
        "metric = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n"
      ],
      "metadata": {
        "id": "g2eI5ASq22P7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer\n",
        "trainer_exp = Trainer(\n",
        "    model_exp.to(\"cuda\"),\n",
        "    args_exp,\n",
        "    train_dataset=expe_train_Hseqs_dataset,\n",
        "    eval_dataset=expe_test_Hseqs_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_lit = Trainer(\n",
        "    model_lit.to(\"cuda\"),\n",
        "    args_lit,\n",
        "    train_dataset=lit_train_Hseqs_dataset,\n",
        "    eval_dataset=lit_test_Hseqs_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "pBFH1E_u3rM6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_exp.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "E9KPc8R54Eic",
        "outputId": "f9028b69-6af2-48ca-8990-1195d7bd9669"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 04:01, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.988116</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.646842</td>\n",
              "      <td>0.131579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.895669</td>\n",
              "      <td>0.263736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.169700</td>\n",
              "      <td>1.179036</td>\n",
              "      <td>0.102564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.169700</td>\n",
              "      <td>1.164823</td>\n",
              "      <td>0.102564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=750, training_loss=0.12545919545491535, metrics={'train_runtime': 242.4429, 'train_samples_per_second': 24.707, 'train_steps_per_second': 3.094, 'total_flos': 954515817023616.0, 'train_loss': 0.12545919545491535, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_lit.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "alTrUH744GFD",
        "outputId": "79ee0488-f056-4fe8-81ac-508aa4e7ee94"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 07:57, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.917929</td>\n",
              "      <td>0.278302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.394900</td>\n",
              "      <td>1.874437</td>\n",
              "      <td>0.313390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.394900</td>\n",
              "      <td>3.238070</td>\n",
              "      <td>0.321060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.206500</td>\n",
              "      <td>3.780032</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.101300</td>\n",
              "      <td>3.837049</td>\n",
              "      <td>0.322086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=0.23421121724446614, metrics={'train_runtime': 478.2175, 'train_samples_per_second': 25.02, 'train_steps_per_second': 3.137, 'total_flos': 1907146300964940.0, 'train_loss': 0.23421121724446614, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Only using CDRH"
      ],
      "metadata": {
        "id": "lr0XjyPjN2Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the data sets from literature and experiment\n",
        "root = \"lemanic_2024/data\"\n",
        "path_exp_train = root + \"/experiment_train.csv\"\n",
        "path_exp_test =  root + \"/experiment_test.csv\"\n",
        "path_lit_train = root + \"/literature_train.csv\"\n",
        "path_lit_test =  root + \"/literature_test.csv\"\n",
        "\n",
        "experiment_train_df = pd.read_csv(path_exp_train).dropna(subset=\"CDRH3\")\n",
        "experiment_test_df = pd.read_csv(path_exp_test).dropna(subset=\"CDRH3\")\n",
        "literature_train_df = pd.read_csv(path_lit_train).dropna(subset=\"CDRH3\")\n",
        "literature_test_df = pd.read_csv(path_lit_test).dropna(subset=\"CDRH3\")\n",
        "\n",
        "experiment_train_df['ID'] = ['id' + str(i) for i in range(1, len(experiment_train_df)+1)]\n",
        "experiment_test_df['ID'] = ['id' + str(i) for i in range(1, len(experiment_test_df)+1)]\n",
        "literature_train_df['ID'] = ['id' + str(i) for i in range(1, len(literature_train_df)+1)]\n",
        "literature_test_df['ID'] = ['id' + str(i) for i in range(1, len(literature_test_df)+1)]"
      ],
      "metadata": {
        "id": "QSmj6S17N4qd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare training seq\n",
        "expe_train_Hseqs = list(experiment_train_df[\"CDRH3\"].values)\n",
        "expe_test_Hseqs = list(experiment_test_df[\"CDRH3\"].values)\n",
        "lit_train_Hseqs = list(literature_train_df[\"CDRH3\"].values)\n",
        "lit_test_Hseqs = list(literature_test_df[\"CDRH3\"].values)\n",
        "\n",
        "def transform_string(s):\n",
        "\n",
        "    return ' '.join(s)\n",
        "\n",
        "expe_train_Hseqs = [transform_string(s) for s in expe_train_Hseqs]\n",
        "expe_test_Hseqs = [transform_string(s) for s in expe_test_Hseqs]\n",
        "lit_train_Hseqs = [transform_string(s) for s in lit_train_Hseqs]\n",
        "lit_test_Hseqs = [transform_string(s) for s in lit_test_Hseqs]\n",
        "\n",
        "#prepare labels\n",
        "y_lit_test = list(np.array(literature_test_df[\"Binds\"].values).astype(int))\n",
        "y_lit_train = list(np.array(literature_train_df[\"Binds\"].values).astype(int))\n",
        "y_exp_test = list(np.array(experiment_test_df[\"Binds\"].values).astype(int))\n",
        "y_exp_train = list(np.array(experiment_train_df[\"Binds\"].values).astype(int))"
      ],
      "metadata": {
        "id": "J2HGCT0HOHoG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare training seq\n",
        "expe_train_Hseqs = list(experiment_train_df[\"CDRH3\"].values)\n",
        "expe_test_Hseqs = list(experiment_test_df[\"CDRH3\"].values)\n",
        "lit_train_Hseqs = list(literature_train_df[\"CDRH3\"].values)\n",
        "lit_test_Hseqs = list(literature_test_df[\"CDRH3\"].values)\n",
        "\n",
        "def transform_string(s):\n",
        "\n",
        "    return ' '.join(s)\n",
        "\n",
        "expe_train_Hseqs = [transform_string(s) for s in expe_train_Hseqs]\n",
        "expe_test_Hseqs = [transform_string(s) for s in expe_test_Hseqs]\n",
        "lit_train_Hseqs = [transform_string(s) for s in lit_train_Hseqs]\n",
        "lit_test_Hseqs = [transform_string(s) for s in lit_test_Hseqs]\n",
        "\n",
        "#prepare labels\n",
        "y_lit_test = list(np.array(literature_test_df[\"Binds\"].values).astype(int))\n",
        "y_lit_train = list(np.array(literature_train_df[\"Binds\"].values).astype(int))\n",
        "y_exp_test = list(np.array(experiment_test_df[\"Binds\"].values).astype(int))\n",
        "y_exp_train = list(np.array(experiment_train_df[\"Binds\"].values).astype(int))"
      ],
      "metadata": {
        "id": "jrkgB3NKOLbN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer and model\n",
        "\n",
        "tokenizer = RoFormerTokenizer.from_pretrained('alchemab/antiberta2')#, truncation=True, max_length=max_len)\n",
        "model_exp = RoFormerForSequenceClassification.from_pretrained('alchemab/antiberta2', num_labels=2) #.resize_token_embeddings(len(tokenizer))\n",
        "model_lit = RoFormerForSequenceClassification.from_pretrained('alchemab/antiberta2', num_labels=2) #.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSXQcM1zOZPy",
        "outputId": "ca4259a8-72d4-4ca4-a120-955b160ea7fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RoFormerForSequenceClassification were not initialized from the model checkpoint at alchemab/antiberta2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RoFormerForSequenceClassification were not initialized from the model checkpoint at alchemab/antiberta2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "expe_train_Hseqs_tokenized = tokenizer(expe_train_Hseqs)\n",
        "expe_test_Hseqs_tokenized = tokenizer(expe_test_Hseqs)\n",
        "\n",
        "lit_train_Hseqs_tokenized = tokenizer(lit_train_Hseqs)\n",
        "lit_test_Hseqs_tokenized = tokenizer(lit_test_Hseqs)"
      ],
      "metadata": {
        "id": "M6EDdAQwOqwV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataset\n",
        "expe_train_Hseqs_dataset = Dataset.from_dict(expe_train_Hseqs_tokenized).add_column(\"labels\", y_exp_train)\n",
        "expe_test_Hseqs_dataset = Dataset.from_dict(expe_test_Hseqs_tokenized).add_column(\"labels\", y_exp_test)\n",
        "\n",
        "lit_train_Hseqs_dataset = Dataset.from_dict(lit_train_Hseqs_tokenized).add_column(\"labels\", y_lit_train)\n",
        "lit_test_Hseqs_dataset = Dataset.from_dict(lit_test_Hseqs_tokenized).add_column(\"labels\", y_lit_test)"
      ],
      "metadata": {
        "id": "aSs-IfirOu0U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train arg\n",
        "batch_size = 8\n",
        "\n",
        "args_exp = TrainingArguments(\n",
        "    f\"AbLang-finetuned-CDRH3-exp\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "args_lit = TrainingArguments(\n",
        "    f\"AbLang-finetuned-CDRH3-lit\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=False,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False\n",
        ")"
      ],
      "metadata": {
        "id": "dRu_PhnWOxlD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics\n",
        "metric = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n"
      ],
      "metadata": {
        "id": "B1JYEtgBOznX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer\n",
        "trainer_exp = Trainer(\n",
        "    model_exp.to(\"cuda\"),\n",
        "    args_exp,\n",
        "    train_dataset=expe_train_Hseqs_dataset,\n",
        "    eval_dataset=expe_test_Hseqs_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_lit = Trainer(\n",
        "    model_lit.to(\"cuda\"),\n",
        "    args_lit,\n",
        "    train_dataset=lit_train_Hseqs_dataset,\n",
        "    eval_dataset=lit_test_Hseqs_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "HaNP3NFIO2Gd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_exp.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "TX23boYtO4cg",
        "outputId": "7756ac87-c2da-4a78-85b0-b1028cd268fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 02:10, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.078328</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.711633</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.791581</td>\n",
              "      <td>0.027778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.242400</td>\n",
              "      <td>1.302083</td>\n",
              "      <td>0.025641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.242400</td>\n",
              "      <td>1.469353</td>\n",
              "      <td>0.027027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=750, training_loss=0.18258979670206707, metrics={'train_runtime': 131.0243, 'train_samples_per_second': 45.717, 'train_steps_per_second': 5.724, 'total_flos': 174668634382248.0, 'train_loss': 0.18258979670206707, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_lit.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hWQVbtydO4op",
        "outputId": "40469b5b-339f-4858-fa2f-7050ab73a8da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:55, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.913672</td>\n",
              "      <td>0.276596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.446700</td>\n",
              "      <td>1.776907</td>\n",
              "      <td>0.282178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.446700</td>\n",
              "      <td>2.144703</td>\n",
              "      <td>0.282421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.301100</td>\n",
              "      <td>3.259022</td>\n",
              "      <td>0.294530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.149500</td>\n",
              "      <td>3.623622</td>\n",
              "      <td>0.293103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=0.29911684163411456, metrics={'train_runtime': 235.5657, 'train_samples_per_second': 50.793, 'train_steps_per_second': 6.368, 'total_flos': 346666684121172.0, 'train_loss': 0.29911684163411456, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}