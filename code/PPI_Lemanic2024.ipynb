{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "xaO5NoCyn65n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adaptyvbio/lemanic_2024.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O250zotuR9uL",
        "outputId": "1c732dfb-8cfb-4010-91d1-67dbc425b690"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lemanic_2024'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 41 (delta 16), reused 36 (delta 13), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (41/41), 1.99 MiB | 4.17 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the data sets from literature and experiment\n",
        "root = \"lemanic_2024/data\"\n",
        "path_exp_train = root + \"/experiment_train.csv\"\n",
        "path_exp_test =  root + \"/experiment_test.csv\"\n",
        "path_lit_train = root + \"/literature_train.csv\"\n",
        "path_lit_test =  root + \"/literature_test.csv\"\n",
        "\n",
        "experiment_train_df = pd.read_csv(path_exp_train)\n",
        "experiment_test_df = pd.read_csv(path_exp_test)\n",
        "literature_train_df = pd.read_csv(path_lit_train)\n",
        "literature_test_df = pd.read_csv(path_lit_test)"
      ],
      "metadata": {
        "id": "V1khYXHX1KIi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_start_indices(df):\n",
        "    start_indices = []\n",
        "    for index, row in df.iterrows():\n",
        "        cdrh3 = row['CDRH3']\n",
        "        vhorvhh = row['VHorVHH']\n",
        "        start_index = vhorvhh.find(cdrh3)\n",
        "        if start_index == -1:\n",
        "          start_index = float('nan')\n",
        "        start_indices.append(start_index)\n",
        "    return start_indices\n",
        "\n",
        "# literature_train\n",
        "literature_train_df[\"start_index_CDRH3\"] = find_start_indices(literature_train_df)\n",
        "experiment_train_df[\"start_index_CDRH3\"] = find_start_indices(experiment_train_df)\n",
        "\n",
        "experiment_test_df[\"start_index_CDRH3\"] = find_start_indices(experiment_test_df)\n",
        "literature_test_df[\"start_index_CDRH3\"] = find_start_indices(literature_test_df)\n",
        "\n",
        "#Stop index\n",
        "experiment_train_df['stop_index_CDRH3'] = experiment_train_df['start_index_CDRH3'] + experiment_train_df['CDRH3'].str.len()\n",
        "literature_train_df['stop_index_CDRH3'] = literature_train_df['start_index_CDRH3'] + literature_train_df['CDRH3'].str.len()\n",
        "\n",
        "literature_test_df['stop_index_CDRH3'] = literature_test_df['start_index_CDRH3'] + literature_test_df['CDRH3'].str.len()\n",
        "experiment_test_df['stop_index_CDRH3'] = experiment_test_df['start_index_CDRH3'] + experiment_test_df['CDRH3'].str.len()"
      ],
      "metadata": {
        "id": "EyUGJEBe3lcP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer and model\n",
        "max_len = 157\n",
        "tokenizer_light = AutoTokenizer.from_pretrained('qilowoq/AbLang_light', truncation=True, max_length=max_len)\n",
        "model_light = AutoModel.from_pretrained('qilowoq/AbLang_light', trust_remote_code=True)\n",
        "\n",
        "tokenizer_heavy = AutoTokenizer.from_pretrained('qilowoq/AbLang_heavy', truncation=True, max_length=max_len)\n",
        "model_heavy = AutoModel.from_pretrained('qilowoq/AbLang_heavy', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "Aq-o_-e-zyDn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_heavy(df, model=model_heavy, tokenizer=tokenizer_heavy):\n",
        "  X = []\n",
        "  for index, row in df.iterrows():\n",
        "\n",
        "    if pd.isna(row['start_index_CDRH3']):\n",
        "      start = 1\n",
        "      end = len(seq) + 1\n",
        "    else:\n",
        "      start = row[\"start_index_CDRH3\"] + 1\n",
        "      end = row[\"stop_index_CDRH3\"] + 1\n",
        "\n",
        "    seq = ' '.join(row['VHorVHH'][0:max_len])\n",
        "    encoded_input = tokenizer(seq, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "      model_output = model(**encoded_input).last_hidden_state\n",
        "\n",
        "\n",
        "    model_output_sliced = model_output[:, int(start):int(end), :]\n",
        "\n",
        "    embedding = model_output_sliced.mean(dim=1)\n",
        "    X.append(embedding.squeeze())\n",
        "\n",
        "  return torch.stack(X).numpy()"
      ],
      "metadata": {
        "id": "no9Ea7QTNTXM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run only if you want to calculate embeddings\n",
        "X_heavy_lit_test = get_embeddings_heavy(literature_test_df)\n",
        "X_heavy_lit_train = get_embeddings_heavy(literature_train_df)\n",
        "\n",
        "X_heavey_exp_train = get_embeddings_heavy(experiment_train_df)\n",
        "X_heavey_exp_test = get_embeddings_heavy(experiment_test_df)"
      ],
      "metadata": {
        "id": "PGvFXPIxbxcx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('X_heavy_lit_test.pkl', 'wb') as f:\n",
        "    pickle.dump(X_heavy_lit_test, f)\n",
        "\n",
        "with open('X_heavy_lit_train.pkl', 'wb') as f:\n",
        "    pickle.dump(X_heavy_lit_train, f)\n",
        "\n",
        "with open('X_heavey_exp_train.pkl', 'wb') as f:\n",
        "    pickle.dump(X_heavey_exp_train, f)\n",
        "\n",
        "with open('X_heavey_exp_test.pkl', 'wb') as f:\n",
        "    pickle.dump(X_heavey_exp_test, f)"
      ],
      "metadata": {
        "id": "6MsTYNlLosJL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('X_heavy_lit_test.pkl', 'rb') as f:\n",
        "    X_heavy_lit_test = pickle.load(f)\n",
        "\n",
        "with open('X_heavy_lit_train.pkl', 'rb') as f:\n",
        "    X_heavy_lit_train = pickle.load(f)\n",
        "\n",
        "with open('X_heavey_exp_train.pkl', 'rb') as f:\n",
        "    X_heavey_exp_train = pickle.load(f)\n",
        "\n",
        "with open('X_heavey_exp_test.pkl', 'rb') as f:\n",
        "    X_heavey_exp_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "k2To1-3qpeuo"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}